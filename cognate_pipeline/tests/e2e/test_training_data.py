"""End-to-end tests for the training data pipeline.

Validates the full-dictionary lexicon files, cognate pair files, and metadata
generated by the training data scripts.
"""

from __future__ import annotations

import csv
from pathlib import Path

import pytest

# Paths relative to the ancient-scripts-datasets repo
REPO_ROOT = Path(__file__).resolve().parent.parent.parent.parent / "ancient-scripts-datasets"
TRAINING_DIR = REPO_ROOT / "data" / "training"
LEXICONS_DIR = TRAINING_DIR / "lexicons"
COGNATE_DIR = TRAINING_DIR / "cognate_pairs"
METADATA_DIR = TRAINING_DIR / "metadata"


def _count_lines(path: Path) -> int:
    """Count non-header lines in a TSV file."""
    if not path.exists():
        return 0
    with open(path, encoding="utf-8") as f:
        return sum(1 for _ in f) - 1  # subtract header


def _read_tsv(path: Path) -> list[dict]:
    """Read a TSV file into list of dicts."""
    if not path.exists():
        return []
    with open(path, encoding="utf-8", newline="") as f:
        return list(csv.DictReader(f, delimiter="\t"))


# ---- Directory existence ----

class TestDirectoryStructure:
    def test_lexicons_dir_exists(self):
        assert LEXICONS_DIR.exists(), f"Lexicons directory not found: {LEXICONS_DIR}"

    def test_cognate_pairs_dir_exists(self):
        assert COGNATE_DIR.exists(), f"Cognate pairs directory not found: {COGNATE_DIR}"

    def test_metadata_dir_exists(self):
        assert METADATA_DIR.exists(), f"Metadata directory not found: {METADATA_DIR}"


# ---- Lexicon file existence and coverage ----

# Top 50 languages expected to have lexicon files
TOP_LANGUAGES = [
    "eng", "fra", "deu", "spa", "por", "ita", "rus", "pol", "jpn",
    "tur", "fin", "hun", "ell", "swe", "dan", "nld", "cat", "ron",
    "ces", "bul", "ukr", "kor", "hin", "ara", "heb", "fas", "tha",
    "vie", "ind", "msa", "lat", "lit", "slv", "hrv", "slk", "nor",
    "isl", "ben", "tam", "tel", "kan", "mal", "mya", "khm", "sqi",
    "hye", "kat", "est", "lav", "eus",
]


class TestLexiconFiles:
    def test_minimum_language_count(self):
        """At least 300 language lexicon files should exist."""
        tsv_files = list(LEXICONS_DIR.glob("*.tsv"))
        assert len(tsv_files) >= 200, (
            f"Expected ≥200 lexicon files, found {len(tsv_files)}"
        )

    @pytest.mark.parametrize("iso", TOP_LANGUAGES)
    def test_top_language_exists(self, iso: str):
        """Each top-50 language should have a lexicon file."""
        path = LEXICONS_DIR / f"{iso}.tsv"
        assert path.exists(), f"Missing lexicon for {iso}"

    def test_english_large(self):
        """English should have ≥50,000 entries."""
        count = _count_lines(LEXICONS_DIR / "eng.tsv")
        assert count >= 50000, f"English has only {count} entries, expected ≥50,000"

    def test_french_large(self):
        """French should have ≥30,000 entries."""
        count = _count_lines(LEXICONS_DIR / "fra.tsv")
        assert count >= 30000, f"French has only {count} entries, expected ≥30,000"

    def test_major_languages_minimum(self):
        """Major languages should have ≥10,000 entries."""
        major = ["eng", "fra", "rus", "deu", "spa", "jpn"]
        for iso in major:
            path = LEXICONS_DIR / f"{iso}.tsv"
            if path.exists():
                count = _count_lines(path)
                assert count >= 5000, (
                    f"{iso} has only {count} entries, expected ≥5,000"
                )


# ---- Lexicon format and quality ----

class TestLexiconQuality:
    def test_header_format(self):
        """All lexicon files should have the correct header."""
        expected = "Word\tIPA\tSCA\tSource\tConcept_ID\tCognate_Set_ID"
        for path in list(LEXICONS_DIR.glob("*.tsv"))[:20]:
            with open(path, encoding="utf-8") as f:
                header = f.readline().strip()
            assert header == expected, (
                f"{path.name}: Wrong header: {header}"
            )

    def test_no_exact_duplicates(self):
        """No duplicate (Word, IPA) rows within any lexicon."""
        # Test a sample of files
        for path in list(LEXICONS_DIR.glob("*.tsv"))[:20]:
            rows = _read_tsv(path)
            seen = set()
            for row in rows:
                key = (row.get("Word", ""), row.get("IPA", ""))
                assert key not in seen, (
                    f"{path.name}: Duplicate entry: {key}"
                )
                seen.add(key)

    def test_sca_quality(self):
        """SCA unknown rate should be < 5% across sampled entries."""
        total_chars = 0
        unknown_chars = 0
        for path in list(LEXICONS_DIR.glob("*.tsv"))[:30]:
            rows = _read_tsv(path)
            for row in rows[:500]:  # sample 500 per language
                sca = row.get("SCA", "")
                total_chars += len(sca)
                unknown_chars += sca.count("0")

        if total_chars > 0:
            unknown_rate = unknown_chars / total_chars
            assert unknown_rate < 0.05, (
                f"SCA unknown rate {unknown_rate:.2%} exceeds 5% threshold"
            )

    def test_entries_have_ipa(self):
        """All entries should have non-empty IPA."""
        for path in list(LEXICONS_DIR.glob("*.tsv"))[:10]:
            rows = _read_tsv(path)
            for row in rows[:200]:
                assert row.get("IPA", "").strip(), (
                    f"{path.name}: Empty IPA for word '{row.get('Word', '')}'"
                )


# ---- Total entry count ----

class TestTotalScale:
    def test_total_entries_minimum(self):
        """Total entries across all lexicons should be ≥1,000,000."""
        total = 0
        for path in LEXICONS_DIR.glob("*.tsv"):
            total += _count_lines(path)
        assert total >= 1000000, (
            f"Total entries {total:,} below 1,000,000 target"
        )


# ---- Cognate pair files ----

class TestCognatePairs:
    def test_inherited_file_exists(self):
        path = COGNATE_DIR / "cognate_pairs_inherited.tsv"
        assert path.exists(), "Missing cognate_pairs_inherited.tsv"

    def test_similarity_file_exists(self):
        path = COGNATE_DIR / "cognate_pairs_similarity.tsv"
        assert path.exists(), "Missing cognate_pairs_similarity.tsv"

    def test_borrowing_file_exists(self):
        path = COGNATE_DIR / "cognate_pairs_borrowing.tsv"
        assert path.exists(), "Missing cognate_pairs_borrowing.tsv"

    def test_inherited_format(self):
        """Inherited pairs file should have correct header."""
        path = COGNATE_DIR / "cognate_pairs_inherited.tsv"
        if path.exists():
            with open(path, encoding="utf-8") as f:
                header = f.readline().strip()
            expected = "Lang_A\tWord_A\tIPA_A\tLang_B\tWord_B\tIPA_B\tConcept_ID\tRelationship\tScore\tSource"
            assert header == expected, f"Wrong header: {header}"

    def test_cognate_pairs_nonempty(self):
        """At least some cognate pairs should exist."""
        total = 0
        for fname in ["cognate_pairs_inherited.tsv", "cognate_pairs_similarity.tsv", "cognate_pairs_borrowing.tsv"]:
            path = COGNATE_DIR / fname
            if path.exists():
                total += _count_lines(path)
        assert total > 0, "No cognate pairs found"


# ---- Metadata files ----

class TestMetadata:
    def test_languages_metadata_exists(self):
        path = METADATA_DIR / "languages.tsv"
        assert path.exists(), "Missing languages.tsv"

    def test_source_stats_exists(self):
        path = METADATA_DIR / "source_stats.tsv"
        assert path.exists(), "Missing source_stats.tsv"
